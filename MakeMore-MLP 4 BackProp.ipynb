{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4226660,"sourceType":"datasetVersion","datasetId":2491115}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random \nrandom.seed(42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/us-baby-names-by-year-of-birth/babyNamesUSYOB-mostpopular.csv')\nnames = df.Name.values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chars = sorted(list(set(''.join(names))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nitos","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"block_size = 3 # context length: how many characters taken to predict the next character","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dataset(words):\n    X, Y = [], []\n    for n in words:\n    \n        context = [0] * block_size\n        for ch in n + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n        \n            context = context[1:] + [ix] # crop and append\n        \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X,Y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(names)\nn1 = int(0.8*len(names))\nn2 = int(0.9*len(names))\n\n# Train, Dev, and Test split \n\nXtr, Ytr = build_dataset(names[:n1])          # 80%\nXdev, Ydev = build_dataset(names[n1:n2])      # 10%\nXte, Yte = build_dataset(names[n2:])          # 10%","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utility function for comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n    ex = torch.all(dt == t.grad).item()\n    app = torch.allclose(dt, t.grad)\n    maxdiff = (dt - t.grad).abs().max().item()\n    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\nloss","metadata":{},"execution_count":null,"outputs":[]}]}
