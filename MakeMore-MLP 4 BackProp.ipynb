{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:21.275719Z","iopub.execute_input":"2024-07-23T19:52:21.276080Z","iopub.status.idle":"2024-07-23T19:52:25.059937Z","shell.execute_reply.started":"2024-07-23T19:52:21.276050Z","shell.execute_reply":"2024-07-23T19:52:25.058838Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import random \nrandom.seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:25.062249Z","iopub.execute_input":"2024-07-23T19:52:25.063105Z","iopub.status.idle":"2024-07-23T19:52:25.068855Z","shell.execute_reply.started":"2024-07-23T19:52:25.063061Z","shell.execute_reply":"2024-07-23T19:52:25.067465Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/us-baby-names-by-year-of-birth/babyNamesUSYOB-mostpopular.csv')\nnames = df.Name.values","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:25.070275Z","iopub.execute_input":"2024-07-23T19:52:25.070742Z","iopub.status.idle":"2024-07-23T19:52:26.505789Z","shell.execute_reply.started":"2024-07-23T19:52:25.070701Z","shell.execute_reply":"2024-07-23T19:52:26.504598Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"chars = sorted(list(set(''.join(names))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nitos","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:26.508563Z","iopub.execute_input":"2024-07-23T19:52:26.508917Z","iopub.status.idle":"2024-07-23T19:52:26.551698Z","shell.execute_reply.started":"2024-07-23T19:52:26.508886Z","shell.execute_reply":"2024-07-23T19:52:26.550479Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{1: 'A',\n 2: 'B',\n 3: 'C',\n 4: 'D',\n 5: 'E',\n 6: 'F',\n 7: 'G',\n 8: 'H',\n 9: 'I',\n 10: 'J',\n 11: 'K',\n 12: 'L',\n 13: 'M',\n 14: 'N',\n 15: 'O',\n 16: 'P',\n 17: 'Q',\n 18: 'R',\n 19: 'S',\n 20: 'T',\n 21: 'U',\n 22: 'V',\n 23: 'W',\n 24: 'X',\n 25: 'Y',\n 26: 'Z',\n 27: 'a',\n 28: 'b',\n 29: 'c',\n 30: 'd',\n 31: 'e',\n 32: 'f',\n 33: 'g',\n 34: 'h',\n 35: 'i',\n 36: 'j',\n 37: 'k',\n 38: 'l',\n 39: 'm',\n 40: 'n',\n 41: 'o',\n 42: 'p',\n 43: 'q',\n 44: 'r',\n 45: 's',\n 46: 't',\n 47: 'u',\n 48: 'v',\n 49: 'w',\n 50: 'x',\n 51: 'y',\n 52: 'z',\n 0: '.'}"},"metadata":{}}]},{"cell_type":"code","source":"block_size = 3 # context length: how many characters taken to predict the next character","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:26.553465Z","iopub.execute_input":"2024-07-23T19:52:26.554493Z","iopub.status.idle":"2024-07-23T19:52:26.562466Z","shell.execute_reply.started":"2024-07-23T19:52:26.554450Z","shell.execute_reply":"2024-07-23T19:52:26.561328Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def build_dataset(words):\n    X, Y = [], []\n    for n in words:\n    \n        context = [0] * block_size\n        for ch in n + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n        \n            context = context[1:] + [ix] # crop and append\n        \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X,Y","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:26.563873Z","iopub.execute_input":"2024-07-23T19:52:26.564288Z","iopub.status.idle":"2024-07-23T19:52:26.575941Z","shell.execute_reply.started":"2024-07-23T19:52:26.564257Z","shell.execute_reply":"2024-07-23T19:52:26.574461Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"random.shuffle(names)\nn1 = int(0.8*len(names))\nn2 = int(0.9*len(names))\n\n# Train, Dev, and Test split \n\nXtr, Ytr = build_dataset(names[:n1])          # 80%\nXdev, Ydev = build_dataset(names[n1:n2])      # 10%\nXte, Yte = build_dataset(names[n2:])          # 10%","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:26.577205Z","iopub.execute_input":"2024-07-23T19:52:26.577752Z","iopub.status.idle":"2024-07-23T19:52:30.908848Z","shell.execute_reply.started":"2024-07-23T19:52:26.577711Z","shell.execute_reply":"2024-07-23T19:52:30.907565Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([1233618, 3]) torch.Size([1233618])\ntorch.Size([154343, 3]) torch.Size([154343])\ntorch.Size([154741, 3]) torch.Size([154741])\n","output_type":"stream"}]},{"cell_type":"code","source":"# utility function for comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n    ex = torch.all(dt == t.grad).item()\n    app = torch.allclose(dt, t.grad)\n    maxdiff = (dt - t.grad).abs().max().item()\n    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:30.910199Z","iopub.execute_input":"2024-07-23T19:52:30.910710Z","iopub.status.idle":"2024-07-23T19:52:30.916863Z","shell.execute_reply.started":"2024-07-23T19:52:30.910680Z","shell.execute_reply":"2024-07-23T19:52:30.915744Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"n_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:30.919689Z","iopub.execute_input":"2024-07-23T19:52:30.920059Z","iopub.status.idle":"2024-07-23T19:52:30.972358Z","shell.execute_reply.started":"2024-07-23T19:52:30.920028Z","shell.execute_reply":"2024-07-23T19:52:30.971141Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"6087\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:30.973722Z","iopub.execute_input":"2024-07-23T19:52:30.974166Z","iopub.status.idle":"2024-07-23T19:52:30.992303Z","shell.execute_reply.started":"2024-07-23T19:52:30.974126Z","shell.execute_reply":"2024-07-23T19:52:30.991170Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n    p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n    t.retain_grad()\nloss.backward()\nloss","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:52:31.101638Z","iopub.execute_input":"2024-07-23T19:52:31.102702Z","iopub.status.idle":"2024-07-23T19:52:31.211816Z","shell.execute_reply.started":"2024-07-23T19:52:31.102660Z","shell.execute_reply":"2024-07-23T19:52:31.210477Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor(4.1605, grad_fn=<NegBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Exercise 1: backprop through the whole thing manually, \n- backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one","metadata":{}},{"cell_type":"code","source":"logprobs.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T19:54:01.154588Z","iopub.execute_input":"2024-07-22T19:54:01.155022Z","iopub.status.idle":"2024-07-22T19:54:01.163048Z","shell.execute_reply.started":"2024-07-22T19:54:01.154989Z","shell.execute_reply":"2024-07-22T19:54:01.161812Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 53])"},"metadata":{}}]},{"cell_type":"markdown","source":"- loss = (a + b + C)/n \n- so dloss/da = -1/n","metadata":{}},{"cell_type":"code","source":"logprobs[range(n), Yb]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T19:55:29.079303Z","iopub.execute_input":"2024-07-22T19:55:29.079761Z","iopub.status.idle":"2024-07-22T19:55:29.090444Z","shell.execute_reply.started":"2024-07-22T19:55:29.079724Z","shell.execute_reply":"2024-07-22T19:55:29.089061Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([-4.4689, -4.0449, -4.9306, -3.4365, -4.5785, -3.9192, -3.4365, -3.6647,\n        -4.4021, -3.5623, -4.2331, -3.1163, -4.9524, -3.9386, -5.0005, -4.6394,\n        -3.8613, -3.4365, -4.1624, -3.4365, -4.1892, -4.1628, -3.6866, -4.4323,\n        -4.5894, -3.9451, -3.9807, -3.5701, -5.5840, -4.5924, -4.4383, -3.9278],\n       grad_fn=<IndexBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"counts.shape , counts_sum_inv.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T20:05:50.150432Z","iopub.execute_input":"2024-07-22T20:05:50.151692Z","iopub.status.idle":"2024-07-22T20:05:50.166641Z","shell.execute_reply.started":"2024-07-22T20:05:50.151651Z","shell.execute_reply":"2024-07-22T20:05:50.165111Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 53]), torch.Size([32, 1]))"},"metadata":{}}]},{"cell_type":"code","source":"norm_logits.shape, logits.shape, logit_maxes.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T20:28:58.386991Z","iopub.execute_input":"2024-07-22T20:28:58.387406Z","iopub.status.idle":"2024-07-22T20:28:58.395100Z","shell.execute_reply.started":"2024-07-22T20:28:58.387373Z","shell.execute_reply":"2024-07-22T20:28:58.393908Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 53]), torch.Size([32, 53]), torch.Size([32, 1]))"},"metadata":{}}]},{"cell_type":"code","source":"plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2024-07-22T20:38:07.134878Z","iopub.execute_input":"2024-07-22T20:38:07.135261Z","iopub.status.idle":"2024-07-22T20:38:07.366579Z","shell.execute_reply.started":"2024-07-22T20:38:07.135232Z","shell.execute_reply":"2024-07-22T20:38:07.365308Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7b144f287dc0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"dlogits.shape, h.shape, W2.shape, b2.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T20:40:40.710719Z","iopub.execute_input":"2024-07-22T20:40:40.711147Z","iopub.status.idle":"2024-07-22T20:40:40.719301Z","shell.execute_reply.started":"2024-07-22T20:40:40.711114Z","shell.execute_reply":"2024-07-22T20:40:40.718165Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 53]),\n torch.Size([32, 64]),\n torch.Size([64, 53]),\n torch.Size([53]))"},"metadata":{}}]},{"cell_type":"markdown","source":"- Can derive a general method from a smaller matrix but can also use the shapes of the matrices as a guide to derive since the derivative is going to have the same shape","metadata":{}},{"cell_type":"code","source":"dh = dlogits @ W2.T \ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T20:48:40.372951Z","iopub.execute_input":"2024-07-22T20:48:40.373588Z","iopub.status.idle":"2024-07-22T20:48:40.386549Z","shell.execute_reply.started":"2024-07-22T20:48:40.373538Z","shell.execute_reply":"2024-07-22T20:48:40.385276Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T20:52:41.538632Z","iopub.execute_input":"2024-07-22T20:52:41.539063Z","iopub.status.idle":"2024-07-22T20:52:41.546800Z","shell.execute_reply.started":"2024-07-22T20:52:41.539028Z","shell.execute_reply":"2024-07-22T20:52:41.545556Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 64]),\n torch.Size([1, 64]),\n torch.Size([32, 64]),\n torch.Size([1, 64]))"},"metadata":{}}]},{"cell_type":"code","source":"bnvar.shape, bnvar_inv.shape, bndiff2.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:11:30.951348Z","iopub.execute_input":"2024-07-22T21:11:30.951832Z","iopub.status.idle":"2024-07-22T21:11:30.959529Z","shell.execute_reply.started":"2024-07-22T21:11:30.951785Z","shell.execute_reply":"2024-07-22T21:11:30.958205Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 64]), torch.Size([1, 64]), torch.Size([32, 64]))"},"metadata":{}}]},{"cell_type":"code","source":"dlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(n), Yb] = -1.0/n\ndprobs = (1.0 / probs) * dlogprobs # chain rule d/dx of log(x) = 1/x  Essentially the lower the probability the greater the multiplication boost\ndcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\ndcounts = counts_sum_inv * dprobs\ndcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\ndcounts += dcounts_sum * torch.ones_like(counts)\ndnorm_logits = counts * dcounts \ndlogits = dnorm_logits.clone() # partial\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\ndh = dlogits @ W2.T\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\ndhpreact = (1.0 - h**2) * dh   # from tanh deriv formula \ndbngain = (bnraw * dhpreact).sum(0, keepdim=True)\ndbnraw = bngain * dhpreact\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbndiff = bnvar_inv * dbnraw\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\ndbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\ndbndiff += (2*bndiff) * dbndiff2\n# dhprebn = dbndiff.clone()\n# dbnmeani = (-dbndiff).sum(0)\n# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n# dembcat = dhprebn @ W1.T\n# dW1 = embcat.T @ dhprebn\n# db1 = dhprebn.sum(0)\n# demb = dembcat.view(emb.shape)\n# dC = torch.zeros_like(C)\n# for k in range(Xb.shape[0]):\n#   for j in range(Xb.shape[1]):\n#     ix = Xb[k,j]\n#     dC[ix] += demb[k,j]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:13:18.175912Z","iopub.execute_input":"2024-07-22T21:13:18.176389Z","iopub.status.idle":"2024-07-22T21:13:18.194616Z","shell.execute_reply.started":"2024-07-22T21:13:18.176351Z","shell.execute_reply":"2024-07-22T21:13:18.193138Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"cmp('logprobs', dlogprobs, logprobs)\ncmp('probs', dprobs, probs)\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\ncmp('counts_sum', dcounts_sum, counts_sum)\ncmp('counts', dcounts, counts)\ncmp('norm_logits', dnorm_logits, norm_logits)\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\ncmp('logits', dlogits, logits)\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\n# cmp('bnmeani', dbnmeani, bnmeani)\n# cmp('hprebn', dhprebn, hprebn)\n# cmp('embcat', dembcat, embcat)\n# cmp('W1', dW1, W1)\n# cmp('b1', db1, b1)\n# cmp('emb', demb, emb)\n# cmp('C', dC, C)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:13:23.645893Z","iopub.execute_input":"2024-07-22T21:13:23.646325Z","iopub.status.idle":"2024-07-22T21:13:23.662433Z","shell.execute_reply.started":"2024-07-22T21:13:23.646293Z","shell.execute_reply":"2024-07-22T21:13:23.660969Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nhpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\nbngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\nbnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbnvar_inv       | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\nbnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\nbndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\nbndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Exercise 2: backprop through cross_entropy but all in one go to complete this challenge look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out\n\n#### forward pass\n\n> before:\n```python\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n```","metadata":{}},{"cell_type":"code","source":"# now:\nloss_fast = F.cross_entropy(logits, Yb)\nprint(loss_fast.item(), 'diff:', (loss_fast - loss).item())","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:22:33.622027Z","iopub.execute_input":"2024-07-22T21:22:33.622679Z","iopub.status.idle":"2024-07-22T21:22:33.642322Z","shell.execute_reply.started":"2024-07-22T21:22:33.622602Z","shell.execute_reply":"2024-07-22T21:22:33.640755Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"4.134999752044678 diff: 0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Backprop\n(Ans)","metadata":{}},{"cell_type":"code","source":"dlogits = F.softmax(logits, 1)\ndlogits[range(n), Yb] -= 1\ndlogits /= n\n\ncmp('logits', dlogits, logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:24:10.637297Z","iopub.execute_input":"2024-07-22T21:24:10.637758Z","iopub.status.idle":"2024-07-22T21:24:10.649737Z","shell.execute_reply.started":"2024-07-22T21:24:10.637721Z","shell.execute_reply":"2024-07-22T21:24:10.647960Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"logits          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n","output_type":"stream"}]},{"cell_type":"code","source":"logits.shape, Yb.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:24:30.417385Z","iopub.execute_input":"2024-07-22T21:24:30.417997Z","iopub.status.idle":"2024-07-22T21:24:30.432614Z","shell.execute_reply.started":"2024-07-22T21:24:30.417946Z","shell.execute_reply":"2024-07-22T21:24:30.431153Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 53]), torch.Size([32]))"},"metadata":{}}]},{"cell_type":"code","source":"F.softmax(logits, 1)[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:24:46.408017Z","iopub.execute_input":"2024-07-22T21:24:46.408457Z","iopub.status.idle":"2024-07-22T21:24:46.418167Z","shell.execute_reply.started":"2024-07-22T21:24:46.408405Z","shell.execute_reply":"2024-07-22T21:24:46.416965Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"tensor([0.0222, 0.0313, 0.0137, 0.0209, 0.0246, 0.0309, 0.0208, 0.0170, 0.0189,\n        0.0142, 0.0131, 0.0115, 0.0186, 0.0144, 0.0115, 0.0313, 0.0123, 0.0163,\n        0.0333, 0.0104, 0.0222, 0.0162, 0.0090, 0.0122, 0.0097, 0.0121, 0.0217,\n        0.0159, 0.0286, 0.0285, 0.0086, 0.0115, 0.0135, 0.0164, 0.0199, 0.0151,\n        0.0305, 0.0447, 0.0225, 0.0286, 0.0281, 0.0162, 0.0068, 0.0220, 0.0329,\n        0.0137, 0.0128, 0.0181, 0.0102, 0.0221, 0.0095, 0.0137, 0.0192],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"dlogits[0] * n","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:25:08.790951Z","iopub.execute_input":"2024-07-22T21:25:08.791355Z","iopub.status.idle":"2024-07-22T21:25:08.801631Z","shell.execute_reply.started":"2024-07-22T21:25:08.791325Z","shell.execute_reply":"2024-07-22T21:25:08.800059Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"tensor([ 0.0222,  0.0313,  0.0137,  0.0209,  0.0246,  0.0309,  0.0208,  0.0170,\n         0.0189,  0.0142,  0.0131,  0.0115,  0.0186,  0.0144,  0.0115,  0.0313,\n         0.0123,  0.0163,  0.0333,  0.0104,  0.0222,  0.0162,  0.0090,  0.0122,\n         0.0097,  0.0121,  0.0217,  0.0159,  0.0286,  0.0285,  0.0086, -0.9885,\n         0.0135,  0.0164,  0.0199,  0.0151,  0.0305,  0.0447,  0.0225,  0.0286,\n         0.0281,  0.0162,  0.0068,  0.0220,  0.0329,  0.0137,  0.0128,  0.0181,\n         0.0102,  0.0221,  0.0095,  0.0137,  0.0192], grad_fn=<MulBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"dlogits[0].sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:25:24.080535Z","iopub.execute_input":"2024-07-22T21:25:24.080976Z","iopub.status.idle":"2024-07-22T21:25:24.089880Z","shell.execute_reply.started":"2024-07-22T21:25:24.080943Z","shell.execute_reply":"2024-07-22T21:25:24.088560Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"tensor(3.7253e-09, grad_fn=<SumBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(dlogits.detach(), cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:25:58.946863Z","iopub.execute_input":"2024-07-22T21:25:58.947260Z","iopub.status.idle":"2024-07-22T21:25:59.179230Z","shell.execute_reply.started":"2024-07-22T21:25:58.947230Z","shell.execute_reply":"2024-07-22T21:25:59.178176Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7b144f0ac4c0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Exercise 3: backprop through batchnorm but all in one go to complete this challenge look at the mathematical expression of the output of batchnorm, take the derivative w.r.t. its input, simplify the expression, and just write it out\n\n#### forward pass\n\n> before:\n```python\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n```","metadata":{}},{"cell_type":"code","source":"# now:\nhpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\nprint('max diff:', (hpreact_fast - hpreact).abs().max())","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:31:01.939872Z","iopub.execute_input":"2024-07-22T21:31:01.941268Z","iopub.status.idle":"2024-07-22T21:31:01.959199Z","shell.execute_reply.started":"2024-07-22T21:31:01.941225Z","shell.execute_reply":"2024-07-22T21:31:01.957764Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### backward pass\n\n>before we had:\n```python\ndbnraw = bngain * dhpreact\ndbndiff = bnvar_inv * dbnraw\ndbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\ndbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\ndbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\ndbndiff += (2*bndiff) * dbndiff2\ndhprebn = dbndiff.clone()\ndbnmeani = (-dbndiff).sum(0)\ndhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n```\n#### calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n(you'll also need to use some of the variables from the forward pass up above)","metadata":{}},{"cell_type":"code","source":"dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0) )\n\ncmp('hprebn', dhprebn, hprebn)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:34:54.434106Z","iopub.execute_input":"2024-07-22T21:34:54.434701Z","iopub.status.idle":"2024-07-22T21:34:54.443757Z","shell.execute_reply.started":"2024-07-22T21:34:54.434655Z","shell.execute_reply":"2024-07-22T21:34:54.442499Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"hprebn          | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\n","output_type":"stream"}]},{"cell_type":"code","source":"dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:35:03.776222Z","iopub.execute_input":"2024-07-22T21:35:03.776677Z","iopub.status.idle":"2024-07-22T21:35:03.785524Z","shell.execute_reply.started":"2024-07-22T21:35:03.776643Z","shell.execute_reply":"2024-07-22T21:35:03.784447Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 64]),\n torch.Size([1, 64]),\n torch.Size([1, 64]),\n torch.Size([32, 64]),\n torch.Size([64]))"},"metadata":{}}]},{"cell_type":"markdown","source":"### Exercise 4: putting it all together!\n#### Train the MLP neural net with your own backward pass\n","metadata":{}},{"cell_type":"code","source":"# init\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nn = batch_size # convenience\nlossi = []\n\n# use this context manager for efficiency once your backward pass is written (TODO)\nwith torch.no_grad():\n\n  # kick off optimization\n  for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmean = hprebn.mean(0, keepdim=True)\n    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n    bnvar_inv = (bnvar + 1e-5)**-0.5\n    bnraw = (hprebn - bnmean) * bnvar_inv\n    hpreact = bngain * bnraw + bnbias\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n      p.grad = None\n    #loss.backward() # use this for correctness comparisons, delete it later!\n\n    # manual backprop!\n    # -----------------\n    dlogits = F.softmax(logits, 1)\n    dlogits[range(n), Yb] -= 1\n    dlogits /= n\n    # 2nd layer backprop\n    dh = dlogits @ W2.T\n    dW2 = h.T @ dlogits\n    db2 = dlogits.sum(0)\n    # tanh\n    dhpreact = (1.0 - h**2) * dh\n    # batchnorm backprop\n    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n    dbnbias = dhpreact.sum(0, keepdim=True)\n    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n    # 1st layer\n    dembcat = dhprebn @ W1.T\n    dW1 = embcat.T @ dhprebn\n    db1 = dhprebn.sum(0)\n    # embedding\n    demb = dembcat.view(emb.shape)\n    dC = torch.zeros_like(C)\n    for k in range(Xb.shape[0]):\n      for j in range(Xb.shape[1]):\n        ix = Xb[k,j]\n        dC[ix] += demb[k,j]\n    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n    # -----------------\n\n    # update\n    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n    for p, grad in zip(parameters, grads):\n      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n      p.data += -lr * grad # new way of swole doge TODO: enable\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:53:07.827430Z","iopub.execute_input":"2024-07-23T19:53:07.827825Z","iopub.status.idle":"2024-07-23T20:01:54.879905Z","shell.execute_reply.started":"2024-07-23T19:53:07.827794Z","shell.execute_reply":"2024-07-23T20:01:54.878760Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"17783\n      0/ 200000: 4.4697\n  10000/ 200000: 1.9537\n  20000/ 200000: 1.7316\n  30000/ 200000: 1.6490\n  40000/ 200000: 1.7280\n  50000/ 200000: 1.5921\n  60000/ 200000: 2.0854\n  70000/ 200000: 1.4935\n  80000/ 200000: 1.8489\n  90000/ 200000: 1.7028\n 100000/ 200000: 1.8807\n 110000/ 200000: 1.3902\n 120000/ 200000: 1.3244\n 130000/ 200000: 1.5188\n 140000/ 200000: 2.1504\n 150000/ 200000: 1.9290\n 160000/ 200000: 1.8636\n 170000/ 200000: 2.0175\n 180000/ 200000: 1.4964\n 190000/ 200000: 1.5058\n","output_type":"stream"}]},{"cell_type":"code","source":"# useful for checking your gradients\n# for p,g in zip(parameters, grads):\n#     cmp(str(tuple(p.shape)), g, p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the bnmean and bnstd for the batch norm at the end of training\n\nwith torch.no_grad():\n    # pass the training set through\n    emb = C[Xtr]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ W1 + b1\n    # measure the mean/std over the entire training set\n    bnmean = hpreact.mean(0, keepdim=True)\n    bnvar = hpreact.var(0, keepdim=True, unbiased=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T20:02:11.897662Z","iopub.execute_input":"2024-07-23T20:02:11.898061Z","iopub.status.idle":"2024-07-23T20:02:15.374134Z","shell.execute_reply.started":"2024-07-23T20:02:11.898028Z","shell.execute_reply":"2024-07-23T20:02:15.372826Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# evaluate train and val loss\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T20:02:15.376421Z","iopub.execute_input":"2024-07-23T20:02:15.376931Z","iopub.status.idle":"2024-07-23T20:02:20.561258Z","shell.execute_reply.started":"2024-07-23T20:02:15.376886Z","shell.execute_reply":"2024-07-23T20:02:20.560206Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"train 1.6158115863800049\nval 1.6194039583206177\n","output_type":"stream"}]},{"cell_type":"code","source":"split_loss('test')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T20:02:52.751701Z","iopub.execute_input":"2024-07-23T20:02:52.752116Z","iopub.status.idle":"2024-07-23T20:02:53.280529Z","shell.execute_reply.started":"2024-07-23T20:02:52.752084Z","shell.execute_reply":"2024-07-23T20:02:53.279291Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"test 1.6182787418365479\n","output_type":"stream"}]},{"cell_type":"code","source":"# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        # ------------\n        # forward pass:\n        # Embedding\n        emb = C[torch.tensor([context])] # (1,block_size,d)      \n        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n        hpreact = embcat @ W1 + b1\n        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n        h = torch.tanh(hpreact) # (N, n_hidden)\n        logits = h @ W2 + b2 # (N, vocab_size)\n        # ------------\n        # Sample\n        probs = F.softmax(logits, dim=1)\n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    \n    print(''.join(itos[i] for i in out))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T20:03:27.622587Z","iopub.execute_input":"2024-07-23T20:03:27.622968Z","iopub.status.idle":"2024-07-23T20:03:27.733408Z","shell.execute_reply.started":"2024-07-23T20:03:27.622936Z","shell.execute_reply":"2024-07-23T20:03:27.732144Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Meghaylonzarian.\nNarolin.\nRayme.\nJosu.\nLauriela.\nDanie.\nJarvell.\nLouianora.\nNina.\nPenily.\nKarib.\nShryn.\nGreg.\nJayson.\nJanie.\nJette.\nAlex.\nBeckeyshuana.\nFanna.\nKalyn.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}